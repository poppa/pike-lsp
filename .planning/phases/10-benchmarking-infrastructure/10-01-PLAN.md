---
phase: 10-benchmarking
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [pike-scripts/analyzer.pike, packages/pike-lsp-server/package.json, packages/pike-lsp-server/benchmarks/runner.ts]
autonomous: true
must_haves:
  truths:
    - "JSON-RPC responses from Pike include an _perf object"
    - "Mitata runner can be invoked via npm/pnpm"
    - "Cold start time is reported in milliseconds"
  artifacts:
    - path: "pike-scripts/analyzer.pike"
      provides: "Internal timing instrumentation"
    - path: "packages/pike-lsp-server/benchmarks/runner.ts"
      provides: "Mitata benchmark entry point"
  key_links:
    - from: "pike-scripts/analyzer.pike"
      to: "JSON-RPC response"
      via: "adding _perf key"
      pattern: "_perf.*System.Timer"
---

<objective>
Enable high-resolution timing inside the Pike process and set up the Mitata benchmark runner in the LSP server.

Purpose: Establish the technical foundation for measuring latency with statistical rigor.
Output: Instrumented Pike analyzer and a functional Mitata runner.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@pike-scripts/analyzer.pike
</context>

<tasks>

<task type="auto">
  <name>Task 1: Instrument Pike analyzer</name>
  <files>pike-scripts/analyzer.pike</files>
  <action>
    Modify `dispatch` in `pike-scripts/analyzer.pike` to:
    - Initialize `System.Timer()` at the start of dispatch.
    - Capture `timer->peek() * 1000` (ms) after the handler returns.
    - Inject an `_perf` mapping into the result (or the root response) containing `pike_total_ms`.
    - Ensure errors still return timing if possible.
    Note: Use `System.Timer` for microsecond resolution; `time()` is too coarse (seconds).
  </action>
  <verify>
    `echo '{"jsonrpc":"2.0","id":1,"method":"get_version","params":{}}' | pike pike-scripts/analyzer.pike`
    Verify output contains `"_perf":{"pike_total_ms": ...}`
  </verify>
  <done>Pike responses include internal timing metadata.</done>
</task>

<task type="auto">
  <name>Task 2: Setup Mitata and Runner</name>
  <files>packages/pike-lsp-server/package.json, packages/pike-lsp-server/benchmarks/runner.ts</files>
  <action>
    - Install `mitata` as a devDependency in `packages/pike-lsp-server`.
    - Add a `benchmark` script to `package.json`: `tsx benchmarks/runner.ts`.
    - Create `packages/pike-lsp-server/benchmarks/runner.ts` with basic Mitata boilerplate.
    - Export a simple `runBenchmarks` function that can be used by CI.
  </action>
  <verify>`pnpm -C packages/pike-lsp-server benchmark` runs without error.</verify>
  <done>Benchmark infrastructure is ready for suites.</done>
</task>

<task type="auto">
  <name>Task 3: Cold Start Benchmark</name>
  <files>packages/pike-lsp-server/benchmarks/runner.ts</files>
  <action>
    Implement a "Cold Start" benchmark in `runner.ts`:
    - Measures `PikeBridge.start()` duration.
    - Measures time to first `introspect` response after start.
    - Uses Mitata's `bench()` to gather statistical data.
  </action>
  <verify>`pnpm -C packages/pike-lsp-server benchmark` reports Cold Start metrics.</verify>
  <done>Cold start baseline is established.</done>
</task>

</tasks>

<verification>
1. Run direct Pike query to see `_perf` data.
2. Run `pnpm benchmark` in the LSP server package and verify output.
</verification>

<success_criteria>
Statistical baseline for cold start is measured and Pike-side latency is transparently reported in IPC.
</success_criteria>

<output>
After completion, create `.planning/phases/10-benchmarking/10-01-SUMMARY.md`
</output>
