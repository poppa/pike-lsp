---
phase: 10-benchmarking
plan: 02
type: execute
wave: 2
depends_on: ["10-01"]
files_modified: [packages/pike-lsp-server/benchmarks/fixtures/small.pike, packages/pike-lsp-server/benchmarks/fixtures/medium.pike, packages/pike-lsp-server/benchmarks/fixtures/large.pike, packages/pike-lsp-server/benchmarks/runner.ts]
autonomous: true
must_haves:
  truths:
    - "Benchmarks exist for small, medium, and large files"
    - "Latency is broken down by End-to-End vs Pike Internal"
    - "Validation, Hover, and Completion are measured"
  artifacts:
    - path: "packages/pike-lsp-server/benchmarks/fixtures/large.pike"
      provides: "Realistic load for benchmarking"
    - path: "packages/pike-lsp-server/benchmarks/runner.ts"
      provides: "Comprehensive feature benchmarks"
  key_links:
    - from: "benchmarks/runner.ts"
      to: "fixtures/*.pike"
      via: "fs.readFileSync"
---

<objective>
Create standard benchmarks for key LSP features using realistic Pike code samples.

Purpose: Establish the performance baseline for individual LSP operations under different load levels.
Output: A comprehensive benchmark suite covering validation, hover, and completion.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-benchmarking/10-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Benchmarking Fixtures</name>
  <files>packages/pike-lsp-server/benchmarks/fixtures/small.pike, packages/pike-lsp-server/benchmarks/fixtures/medium.pike, packages/pike-lsp-server/benchmarks/fixtures/large.pike</files>
  <action>
    Create realistic Pike code fixtures:
    - `small.pike`: ~10 lines, single class, 1 method.
    - `medium.pike`: ~100 lines, multiple classes, imports, varied symbols.
    - `large.pike`: ~1000 lines, complex inheritance, many methods, deep nesting.
    Use snippets from existing Pike stdlib or previous tests as templates.
  </action>
  <verify>Check that files exist and are valid Pike code.</verify>
  <done>Fixtures are available for benchmarking.</done>
</task>

<task type="auto">
  <name>Task 2: Implement Validation Suite</name>
  <files>packages/pike-lsp-server/benchmarks/runner.ts</files>
  <action>
    Add a "Validation" group to Mitata runner:
    - Measures the combined cost of: `introspect` + `parse` + `analyze_uninitialized` (current validation flow).
    - Benchmarks against small, medium, and large fixtures.
    - Extracts `pike_total_ms` from `_perf` metadata to show analysis vs IPC overhead.
  </action>
  <verify>`pnpm benchmark` shows Validation results for all 3 fixtures.</verify>
  <done>Validation latency is baselined.</done>
</task>

<task type="auto">
  <name>Task 3: Implement Intelligence Suite</name>
  <files>packages/pike-lsp-server/benchmarks/runner.ts</files>
  <action>
    Add an "Intelligence" group to Mitata runner:
    - **Hover**: Benchmark `resolve` and `resolve_stdlib` for common types.
    - **Completion**: Benchmark `get_completion_context` at various points in the `large.pike` fixture.
    - Report both E2E time and Pike-internal time.
  </action>
  <verify>`pnpm benchmark` shows Hover and Completion results.</verify>
  <done>Intelligence request latency is baselined.</done>
</task>

</tasks>

<verification>
1. Run `pnpm benchmark` and verify that all suites (Validation, Hover, Completion) execute and produce p99/variance data.
2. Verify the output includes the Pike-side internal timing breakdown.
</verification>

<success_criteria>
Benchmarks cover all primary LSP operations across three file sizes, with internal timing data for bottleneck identification.
</success_criteria>

<output>
After completion, create `.planning/phases/10-benchmarking/10-02-SUMMARY.md`
</output>
