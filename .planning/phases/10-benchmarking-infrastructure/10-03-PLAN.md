---
phase: 10-benchmarking
plan: 03
type: execute
wave: 3
depends_on: ["10-02"]
files_modified: [scripts/benchmark-ci.sh, .github/workflows/bench.yml, .planning/BENCHMARKS.md]
autonomous: true
must_haves:
  truths:
    - "CI automatically runs benchmarks on push/PR"
    - "CI fails if performance regresses beyond 20%"
    - "Performance trends are visualized in a dashboard (via action)"
  artifacts:
    - path: "scripts/benchmark-ci.sh"
      provides: "CI-compatible benchmark runner"
    - path: ".github/workflows/bench.yml"
      provides: "CI automation for performance tracking"
    - path: ".planning/BENCHMARKS.md"
      provides: "Documentation for running and interpreting benchmarks"
---

<objective>
Automate benchmark execution and report generation in GitHub Actions to prevent performance regressions.

Purpose: Ensure that future optimizations are measurable and that new features do not accidentally degrade performance.
Output: CI automation and documentation for the benchmarking infrastructure.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-benchmarking/10-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: CI Scripting</name>
  <files>scripts/benchmark-ci.sh</files>
  <action>
    Create `scripts/benchmark-ci.sh`:
    - Navigates to `packages/pike-lsp-server`.
    - Runs the Mitata benchmark suite with JSON output format.
    - Captures the output to a file (e.g., `benchmark-results.json`).
    - Ensures the script returns a non-zero exit code if the benchmark process itself fails.
  </action>
  <verify>`bash scripts/benchmark-ci.sh` executes and produces a JSON result file.</verify>
  <done>Benchmarks can be run in a non-interactive CI environment.</done>
</task>

<task type="auto">
  <name>Task 2: GitHub Actions Integration</name>
  <files>.github/workflows/bench.yml</files>
  <action>
    Create or update `.github/workflows/bench.yml`:
    - Triggers on `push` to `main` and on `pull_request`.
    - Sets up Node.js and Pike environments.
    - Runs `scripts/benchmark-ci.sh`.
    - Uses `benchmark-action/github-action-benchmark` to:
      - Compare results against the previous run on `main`.
      - Fail the build if a regression > 20% is detected.
      - Update a performance dashboard on a dedicated branch (e.g., `gh-pages` or `benchmark-data`).
  </action>
  <verify>Check GitHub Actions configuration syntax and workflow logic.</verify>
  <done>Performance regressions are automatically caught in CI.</done>
</task>

<task type="auto">
  <name>Task 3: Benchmark Documentation</name>
  <files>.planning/BENCHMARKS.md</files>
  <action>
    Create `.planning/BENCHMARKS.md`:
    - Explain how to run benchmarks locally (`pnpm benchmark`).
    - Describe the different suites (Cold Start, Validation, Intelligence).
    - Explain the `_perf` metadata and how to interpret analysis vs IPC overhead.
    - Document the regression threshold and CI behavior.
  </action>
  <verify>Read the file to ensure clarity and completeness.</verify>
  <done>Benchmarking knowledge is shared and documented.</done>
</task>

</tasks>

<verification>
1. Verify `scripts/benchmark-ci.sh` permissions and execution.
2. Verify `.github/workflows/bench.yml` presence and configuration.
3. Review `.planning/BENCHMARKS.md` content.
</verification>

<success_criteria>
Automated performance tracking is active, with clear documentation for developers to follow when adding new features or optimizations.
</success_criteria>

<output>
After completion, create `.planning/phases/10-benchmarking/10-03-SUMMARY.md`
</output>
